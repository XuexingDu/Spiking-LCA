{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spirit/anaconda3/envs/cy_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3,4\" # specify which GPU(s) to be used\n",
    "bm.disable_gpu_memory_preallocation()\n",
    "bm.set_platform('gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10, Best Fitness: 2979.6792\n",
      "Generation 20, Best Fitness: 2936.1721\n",
      "Generation 30, Best Fitness: 2915.5439\n",
      "Generation 40, Best Fitness: 2895.5864\n",
      "Generation 50, Best Fitness: 2861.0172\n",
      "Generation 60, Best Fitness: 2896.3543\n",
      "Generation 70, Best Fitness: 2869.5685\n",
      "Generation 80, Best Fitness: 2843.6312\n",
      "Generation 90, Best Fitness: 2838.0723\n",
      "Generation 100, Best Fitness: 2836.3082\n",
      "Generation 110, Best Fitness: 2828.5495\n",
      "Generation 120, Best Fitness: 2829.9187\n",
      "Generation 130, Best Fitness: 2816.4858\n",
      "Generation 140, Best Fitness: 2795.5762\n",
      "Generation 150, Best Fitness: 2801.0294\n",
      "Generation 160, Best Fitness: 2780.2300\n",
      "Generation 170, Best Fitness: 2783.5249\n",
      "Generation 180, Best Fitness: 2781.5139\n",
      "Generation 190, Best Fitness: 2758.9276\n",
      "Generation 200, Best Fitness: 2786.6119\n",
      "Generation 210, Best Fitness: 2777.4903\n",
      "Generation 220, Best Fitness: 2769.8142\n",
      "Generation 230, Best Fitness: 2769.5303\n",
      "Generation 240, Best Fitness: 2776.6692\n",
      "Generation 250, Best Fitness: 2765.4581\n",
      "Generation 260, Best Fitness: 2749.4131\n",
      "Generation 270, Best Fitness: 2750.6024\n",
      "Generation 280, Best Fitness: 2738.1272\n",
      "Generation 290, Best Fitness: 2718.7420\n",
      "Generation 300, Best Fitness: 2734.1115\n",
      "Generation 310, Best Fitness: 2725.7489\n",
      "Generation 320, Best Fitness: 2706.4149\n",
      "Generation 330, Best Fitness: 2703.0427\n",
      "Generation 340, Best Fitness: 2706.2875\n",
      "Generation 350, Best Fitness: 2710.2638\n",
      "Generation 360, Best Fitness: 2707.0091\n",
      "Generation 370, Best Fitness: 2708.0058\n",
      "Generation 380, Best Fitness: 2709.8447\n",
      "Generation 390, Best Fitness: 2713.5054\n",
      "Generation 400, Best Fitness: 2706.0498\n",
      "Generation 410, Best Fitness: 2698.9287\n",
      "Generation 420, Best Fitness: 2698.1930\n",
      "Generation 430, Best Fitness: 2707.5603\n",
      "Generation 440, Best Fitness: 2699.7670\n",
      "Generation 450, Best Fitness: 2714.2454\n",
      "Generation 460, Best Fitness: 2707.2650\n",
      "Generation 470, Best Fitness: 2694.7690\n",
      "Generation 480, Best Fitness: 2695.0814\n",
      "Generation 490, Best Fitness: 2698.4751\n",
      "Generation 500, Best Fitness: 2705.4714\n",
      "Generation 510, Best Fitness: 2709.5768\n",
      "Generation 520, Best Fitness: 2705.3687\n",
      "Generation 530, Best Fitness: 2705.2292\n",
      "Generation 540, Best Fitness: 2705.4898\n",
      "Generation 550, Best Fitness: 2702.1213\n",
      "Generation 560, Best Fitness: 2711.6306\n",
      "Generation 570, Best Fitness: 2688.0869\n",
      "Generation 580, Best Fitness: 2706.0526\n",
      "Generation 590, Best Fitness: 2713.6690\n",
      "Generation 600, Best Fitness: 2719.0618\n",
      "Generation 610, Best Fitness: 2740.5281\n",
      "Generation 620, Best Fitness: 2734.9433\n",
      "Generation 630, Best Fitness: 2735.6676\n",
      "Generation 640, Best Fitness: 2733.5057\n",
      "Generation 650, Best Fitness: 2745.7375\n",
      "Generation 660, Best Fitness: 2732.8073\n",
      "Generation 670, Best Fitness: 2714.3950\n",
      "Generation 680, Best Fitness: 2724.4793\n",
      "Generation 690, Best Fitness: 2724.8958\n",
      "Generation 700, Best Fitness: 2733.1417\n",
      "Generation 710, Best Fitness: 2721.6503\n",
      "Generation 720, Best Fitness: 2717.1353\n",
      "Generation 730, Best Fitness: 2701.1812\n",
      "Generation 740, Best Fitness: 2719.1836\n",
      "Generation 750, Best Fitness: 2725.6641\n",
      "Generation 760, Best Fitness: 2721.5738\n",
      "Generation 770, Best Fitness: 2720.1576\n",
      "Generation 780, Best Fitness: 2717.1130\n",
      "Generation 790, Best Fitness: 2700.0523\n",
      "Generation 800, Best Fitness: 2717.9627\n",
      "Generation 810, Best Fitness: 2717.4788\n",
      "Generation 820, Best Fitness: 2724.2875\n",
      "Generation 830, Best Fitness: 2705.5675\n",
      "Generation 840, Best Fitness: 2703.0968\n",
      "Generation 850, Best Fitness: 2714.8152\n",
      "Generation 860, Best Fitness: 2716.3336\n",
      "Generation 870, Best Fitness: 2712.3189\n",
      "Generation 880, Best Fitness: 2713.2269\n",
      "Generation 890, Best Fitness: 2693.8313\n",
      "Generation 900, Best Fitness: 2716.3008\n",
      "Generation 910, Best Fitness: 2699.2805\n",
      "Generation 920, Best Fitness: 2696.3986\n",
      "Generation 930, Best Fitness: 2711.4090\n",
      "Generation 940, Best Fitness: 2703.3313\n",
      "Generation 950, Best Fitness: 2711.1978\n",
      "Generation 960, Best Fitness: 2698.1142\n",
      "Generation 970, Best Fitness: 2704.5090\n",
      "Generation 980, Best Fitness: 2727.6289\n",
      "Generation 990, Best Fitness: 2715.0557\n",
      "Generation 1000, Best Fitness: 2705.3022\n",
      "Genetic Algorithm Solution:\n",
      "[0.         0.         0.         ... 0.         0.46194857 0.        ]\n"
     ]
    }
   ],
   "source": [
    "def genetic_algorithm(A, b, population_size=500, generations=1000, mutation_rate=0.1, crossover_rate=0.7, sparsity=10):\n",
    "    \"\"\"\n",
    "    Genetic Algorithm for Sparse Coding\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Initialize population\n",
    "    population = np.random.randn(population_size, n)\n",
    "    for i in range(population_size):\n",
    "        # Enforce sparsity by zeroing out random coefficients\n",
    "        zero_indices = np.random.choice(n, n - sparsity, replace=False)\n",
    "        population[i, zero_indices] = 0\n",
    "\n",
    "    for gen in range(generations):\n",
    "        # Evaluate fitness (negative reconstruction error plus sparsity penalty)\n",
    "        fitness = -np.linalg.norm(A @ population.T - b.reshape(-1, 1), axis=0) - 0.01 * np.count_nonzero(population, axis=1)\n",
    "\n",
    "        # Selection (roulette wheel selection)\n",
    "        fitness_shifted = fitness - fitness.min() + 1e-6  # Ensure positive fitness\n",
    "        probabilities = fitness_shifted / fitness_shifted.sum()\n",
    "        indices = np.random.choice(population_size, size=population_size, p=probabilities)\n",
    "        selected_population = population[indices]\n",
    "\n",
    "        # Crossover\n",
    "        new_population = []\n",
    "        for i in range(0, population_size, 2):\n",
    "            parent1 = selected_population[i]\n",
    "            parent2 = selected_population[(i+1) % population_size]\n",
    "            if np.random.rand() < crossover_rate:\n",
    "                crossover_point = np.random.randint(1, n)\n",
    "                child1 = np.hstack((parent1[:crossover_point], parent2[crossover_point:]))\n",
    "                child2 = np.hstack((parent2[:crossover_point], parent1[crossover_point:]))\n",
    "            else:\n",
    "                child1, child2 = parent1.copy(), parent2.copy()\n",
    "            new_population.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for individual in new_population:\n",
    "            if np.random.rand() < mutation_rate:\n",
    "                mutation_indices = np.random.choice(n, sparsity // 2, replace=False)\n",
    "                individual[mutation_indices] += np.random.randn(mutation_indices.size)\n",
    "\n",
    "        # Enforce sparsity\n",
    "        for individual in new_population:\n",
    "            sorted_indices = np.argsort(np.abs(individual))\n",
    "            individual[sorted_indices[:-sparsity]] = 0\n",
    "\n",
    "        population = np.array(new_population)\n",
    "\n",
    "        # Optional: Print best fitness every few generations\n",
    "        if (gen + 1) % 10 == 0:\n",
    "            best_fitness = -fitness.max()\n",
    "            print(f\"Generation {gen + 1}, Best Fitness: {best_fitness:.4f}\")\n",
    "\n",
    "    # Return the best individual\n",
    "    fitness = -np.linalg.norm(A @ population.T - b.reshape(-1, 1), axis=0) - 0.01 * np.count_nonzero(population, axis=1)\n",
    "    best_index = np.argmax(fitness)\n",
    "    best_solution = population[best_index]\n",
    "    return best_solution\n",
    "\n",
    "\n",
    "def differential_evolution_sparse(A, b, lam, pop_size=50, max_iter=1000, F=0.8, CR=0.9):\n",
    "    \"\"\"\n",
    "    Differential Evolution for Sparse Coding\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    # Objective function\n",
    "    def objective(x):\n",
    "        return np.linalg.norm(A @ x - b) ** 2 + lam * np.linalg.norm(x, 1)\n",
    "\n",
    "    # Initialize population\n",
    "    pop = np.random.randn(pop_size, n)\n",
    "    # Evaluate initial population\n",
    "    fitness = np.array([objective(ind) for ind in pop])\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        for i in range(pop_size):\n",
    "            # Mutation\n",
    "            idxs = [idx for idx in range(pop_size) if idx != i]\n",
    "            a, b_ind, c = pop[np.random.choice(idxs, 3, replace=False)]\n",
    "            mutant = a + F * (b_ind - c)\n",
    "            # Crossover\n",
    "            cross_points = np.random.rand(n) < CR\n",
    "            if not np.any(cross_points):\n",
    "                cross_points[np.random.randint(0, n)] = True\n",
    "            trial = np.where(cross_points, mutant, pop[i])\n",
    "            # Selection\n",
    "            f = objective(trial)\n",
    "            if f < fitness[i]:\n",
    "                pop[i] = trial\n",
    "                fitness[i] = f\n",
    "\n",
    "        # Optional: Print progress\n",
    "        if (iter + 1) % 100 == 0:\n",
    "            print(f\"Iteration {iter + 1}, Best Fitness: {fitness.min():.4f}\")\n",
    "\n",
    "    # Return the best individual\n",
    "    best_idx = np.argmin(fitness)\n",
    "    best_solution = pop[best_idx]\n",
    "    return best_solution\n",
    "\n",
    "def particle_swarm_optimization_sparse(A, b, lam, pop_size=30, max_iter=1000, w=0.7, c1=1.5, c2=1.5):\n",
    "    \"\"\"\n",
    "    Particle Swarm Optimization for Sparse Coding\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    # Objective function\n",
    "    def objective(x):\n",
    "        return np.linalg.norm(A @ x - b) ** 2 + lam * np.linalg.norm(x, 1)\n",
    "\n",
    "    # Initialize particles\n",
    "    x = np.random.randn(pop_size, n)\n",
    "    v = np.zeros((pop_size, n))\n",
    "    p_best = x.copy()\n",
    "    p_best_val = np.array([objective(ind) for ind in x])\n",
    "    g_best_idx = np.argmin(p_best_val)\n",
    "    g_best = p_best[g_best_idx].copy()\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        for i in range(pop_size):\n",
    "            r1 = np.random.rand(n)\n",
    "            r2 = np.random.rand(n)\n",
    "            # Update velocity\n",
    "            v[i] = w * v[i] + c1 * r1 * (p_best[i] - x[i]) + c2 * r2 * (g_best - x[i])\n",
    "            # Update position\n",
    "            x[i] = x[i] + v[i]\n",
    "            # Apply soft thresholding for sparsity\n",
    "            x[i] = np.sign(x[i]) * np.maximum(np.abs(x[i]) - lam * 0.1, 0)\n",
    "            # Evaluate\n",
    "            f = objective(x[i])\n",
    "            # Update personal best\n",
    "            if f < p_best_val[i]:\n",
    "                p_best[i] = x[i].copy()\n",
    "                p_best_val[i] = f\n",
    "                # Update global best\n",
    "                if f < p_best_val[g_best_idx]:\n",
    "                    g_best = x[i].copy()\n",
    "                    g_best_idx = i\n",
    "\n",
    "        # Optional: Print progress\n",
    "        if (iter + 1) % 100 == 0:\n",
    "            print(f\"Iteration {iter + 1}, Best Fitness: {p_best_val[g_best_idx]:.4f}\")\n",
    "    return g_best\n",
    "\n",
    "m, n = 5000, 10000\n",
    "A = np.random.randn(m, n)\n",
    "sparsity_value = int(0.1 * n)\n",
    "x_true = np.zeros(n)\n",
    "non_zero_indices = np.random.choice(n, sparsity_value, replace=False)\n",
    "x_true[non_zero_indices] = np.random.randn(sparsity_value)\n",
    "b = A @ x_true + 0.1 * np.random.randn(m)\n",
    "\n",
    "x_ga = genetic_algorithm(A, b, sparsity=sparsity_value)\n",
    "print(\"Genetic Algorithm Solution:\")\n",
    "print(x_ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
